
DBManager.py

    1. 기존에 있던 코드들을 정리하고 필요 없다고 생각한 함수를 불용처리함

    2. 쿼리 실행 단계에서 cursor.close()를 실행하도록 변경함

    3. 함수에 SQL injection 방지 추가

        execute(sql, value) 따로 집어넣어서 자동으로 처리됨
        pymysql(.converter).escape_string(sql) 은 사용 안됨; 이유가...?

    0219 작성
    4. 중복 데이터 방지 추가

        insert_df() 함수의 INSERT 구문 수정 > INSERT IGNORE table ~
        테이블에 UNIQUE KEY 추가 > (name, code, date, title, link)

===========================================================================================

discussion_list_crawling.py

    1. 링크가 필요한가? 새로운 글이 작성되면 페이지가 밀림 > 이전에 저장한 링크가 필요 없어짐

        URL에서 nid가 있다면 그 뒷 줄은 필요없음. 크롤링 해오는 주소를 보완하고 뒷쪽을 지워버림
        이후 게시글을 크롤링 해오는 과정에서 사용할 수 있는 URL을 완성함

    2. 아직 다수의 데이터를 집어넣는 테스트는 안해봄

        해볼것

        0217 작성
        페이지가 100 초과 즉, 101 페이지부터 정상적인 접근이 아님을 인지하고 막아버림
        기존의 driver.get(url)은 막히기 때문에 driver.execute_script(url)로 우회해서 크롤링 실행

        삼성전자 같이 종목토론방에 데이터가 너무 많으면 페이지를 크게 넘어가는 로직도 구현
        찾고자하는 날짜와 현재 날짜의 일수차를 계산하고 빅스텝 -> 스몰스텝으로 자연스럽게 줄어듦
        완전하지는 않지만 일부 데이터 손실을 감안해도 될정도라고 생각하고 진행중(게시글 몇개 잘릴 수 있음)

        그러나 찾고자하는 날짜가 나타나고 게시물이 많은만큼 크롤링 속도가 현저하게 낮음
        한번만 하면 되는 작업이라서 큰 장애물이라고 생각하지는 않음

    0218 작성
    3. 무한루프에 빠짐

        날짜 로직에 문제가 있어서 페이지를 순회하는 무한루프에 빠짐
        다시 로직을 수정해서 무한루프 문제는 해결함


    0219 작성
    4. 중복된 데이터가 생각보다 자주 들어감 > 삼성전자 19만건 중 141개

        DBManager.py에서 조치를 취함

===========================================================================================

discussion_post_crawling.py

    1. 데이터베이스에서 테이블 설계가 복잡함

        복잡하지만 최대한 단순화 하는중. 테이블 하나로 끝내는 방법도 고려중임
        disc_analysis _cont 컬럼은 left(~_cont, N)으로 잘라보기 필수

        0217 작성
        데이터베이스에서 테이블을 다시 설계함 (mysql.txt 0217 수정 참조)
        전체 테이블을 세개로 줄이면서 discussion에 해당하는 테이블은 한개로 줄임
        SQL문을 통해 잘 조절해야 함

    2. 병렬처리(스레드)를 도입해서 크롤링에 걸리는 시간 단축

        완벽히 이해하지는 못했으나 하나의 웹 드라이버로 10개의 스레드를 동시에 처리함
        다른 파일에서도 최적화하는 방법을 찾아서 도입하면 로딩 시간을 줄일 수 있을듯

    3. 게시글 추가, 삭제가 빈번함

        같은 날짜를 사흘에 거쳐서 반복 수집하니 99개 -> 89개 -> 90개로 날마다 변함
        수집한 link 데이터가 필요없어지는 경우가 발생하여 failed_url 리스트 만듦
        수집하는 데이터보다도 나중에 보여질 데이터의 신뢰성에 영향을 미칠 수도 있을듯 싶음

        해결하기 위해서 변동이 적은 과거의 데이터를 수집한다면?
        일주일에서 한달 전 데이터는 변동이 적을 것으로 예상하지만,
        그만큼 최근의 심리지수는 파악할 수 없다는 맹점이 있음

        그럼 한달간의 데이터만 가지고 쓴다면?
        데이터가 쌓이는 형식이 아니라 한개의 데이터가 들어오면 한개의 데이터가 나가는 형식
        30일분의 데이터만 가지고 매일매일 30일분의 데이터를 갱신한다면 어떨까?

        0217 작성
        일부 데이터 손실을 감안하더라도 게시글이 워낙 많기 때문에 감안하기로 결정
        큰 영향은 미치지 않을 것으로 예상되기도 하고 과거로 갈 수록 데이터 손실 정도가 줄어들 수
        있기 때문에 그 영향이 더 작을 것으로 예상됨

    0220 작성
    4. 오류 발생으로 크롤링 실행 중에 꺼짐

        가장 오래 걸리는 작업임에도 불구하고 실행시키고 나면 크롤링 진행중에 꺼져버림
        오류 메세지 분석결과 크롤링 처리시 메모리 사용 방식에 문제가 있는 것을 발견
        드라이버 로드 방식을 주기적으로 quit() 시키는 방식으로 바꾸고
        데이터 프레임에서 컬럼을 꺼내오는 방식을 iterrow()에서 itertuple()로 바꿈

===========================================================================================

discussion_post_crawling.py

    0217 작성
    1. 감성분석 모델을 아직 못 정함

        나중에 교체하기 쉽도록 데이터 전처리 파트와 감성분석 파트를 나눠서 함수로 뒀으나 파인튜닝에
        사용할 데이터셋 찾기가 쉽지 않음. 파인튜닝에 사용할 pre-trained 모델은 KcELECTRA를 염두에
        두고 있으나 튜닝할 방법도 잘 모르겠음

        일단 기존 모델을 쓰고 데이터만 만들어놓은 다음에 웹페이지 구현까지 끝내놓고 모델을 탐색할 예정***

    0219 작성
    2. 특수문자를 제외해야 함

        특수문자가 들어간 경우 의미없는 글이거나 광고성, 데이터로써 의미없는 글인 경우가 많기 때문에
        분석할 리스트에서 전부 제외하도록 설정함. 허용할 특수문자도 일부 지정했으나 좀 더 디테일한
        지정이 필요해 보임

===========================================================================================

run_all.py

    0219 작성
    1. discussion_list_crawling.py, discussion_post_crawling.py, discussion_sentiment.py 세 파일 동시실행 끊김

        무한루프에 빠지는 등 수정했으나 데이터 23만건을 추가하고 discussion_list_crawling.py를 실행하고 끊겨버림
        이유를 알 수 없으나 일단 stock_list에서 삼성전자를 제외. 23만건의 데이터 중 19만건이 삼성전자 데이터임
        삼성전자를 제외하는 대신 리스트를 10개로 늘림. 다시 가동해보고 끊기는지 확인 필요

    2. 날짜 인자를 넘겨줄 수 있는 부분이 없음

        subprocess로 실행하면 해당 py파일을 main으로 진행하기 때문에(import와 다름) 시험을 위한 'if __name__ == "__main__"
        구문이 동작함. 따라서 run_all.py에서 날짜 변수를 지정하고 *args로 다음 파일에 넘겨주도록 함